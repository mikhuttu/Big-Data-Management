% This is "sig-alternate.tex" V2.1 April 2013
% This file should be compiled with V2.5 of "sig-alternate.cls" May 2012
%
% This example file demonstrates the use of the 'sig-alternate.cls'
% V2.5 LaTeX2e document class file. It is for those submitting
% articles to ACM Conference Proceedings WHO DO NOT WISH TO
% STRICTLY ADHERE TO THE SIGS (PUBS-BOARD-ENDORSED) STYLE.
% The 'sig-alternate.cls' file will produce a similar-looking,
% albeit, 'tighter' paper resulting in, invariably, fewer pages.
%
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V2.5) produces:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) NO page numbers
%
% as against the acm_proc_article-sp.cls file which
% DOES NOT produce 1) thru' 3) above.
%
% Using 'sig-alternate.cls' you have control, however, from within
% the source .tex file, over both the CopyrightYear
% (defaulted to 200X) and the ACM Copyright Data
% (defaulted to X-XXXXX-XX-X/XX/XX).
% e.g.
% \CopyrightYear{2007} will cause 2007 to appear in the copyright line.
% \crdata{0-12345-67-8/90/12} will cause 0-12345-67-8/90/12 to appear in the copyright line.
%
% ---------------------------------------------------------------------------------------------------------------
% This .tex source is an example which *does* use
% the .bib file (from which the .bbl file % is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission, you *NEED* to 'insert'
% your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% ================= IF YOU HAVE QUESTIONS =======================
% Questions regarding the SIGS styles, SIGS policies and
% procedures, Conferences etc. should be sent to
% Adrienne Griscti (griscti@acm.org)
%
% Technical questions _only_ to
% Gerald Murray (murray@hq.acm.org)
% ===============================================================
%
% For tracking purposes - this is V2.0 - May 2012

\documentclass{sig-alternate-05-2015}

\begin{document}

\title{Big data cleansing based on qualitative attributes}

\numberofauthors{1}

\author{
\alignauthor
	Mika Huttunen\\
       		\affaddr{Helsinki University}
}

\date{11 March 2018}

\maketitle

\begin{abstract}

This paper provides a sample of a \LaTeX\ document which conforms,
somewhat loosely, to the formatting guidelines for
ACM SIG Proceedings. It is an {\em alternate} style which produces
a {\em tighter-looking} paper and was designed in response to
concerns expressed, by authors, over page-budgets.
It complements the document \textit{Author's (Alternate) Guide to
Preparing ACM SIG Proceedings Using \LaTeX$2_\epsilon$\ and Bib\TeX}.
This source file has been written with the intention of being
compiled under \LaTeX$2_\epsilon$\ and BibTeX.

\end{abstract}

%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below. 
%
%\begin{CCSXML}
%<ccs2012>
% <concept>
%  <concept_id>10010520.10010553.10010562</concept_id>
%  <concept_desc>Computer systems organization~Embedded systems</concept_desc>
%  <concept_significance>500</concept_significance>
% </concept>
% <concept>
%  <concept_id>10010520.10010575.10010755</concept_id>
%  <concept_desc>Computer systems organization~Redundancy</concept_desc>
%  <concept_significance>300</concept_significance>
% </concept>
%</ccs2012>  
%\end{CCSXML}

%\ccsdesc[500]{Computer systems organization~Embedded systems}
%\ccsdesc[300]{Computer systems organization~Redundancy}

\printccsdesc

\keywords{big data, data cleansing, data analysis, data transformation}


% 1. INTRODUCTION
\section{Introduction}

Nowadays companies are gathering large amounts of data, and its becoming easier to use data to help their businesses in all kinds of
decision making, analytics, or for example automating human-involved tasks.
Data can be collected in different ways like via user input and, all kinds of sensors.
Humans often make data input errors via misspelling, and sensors may grab unwanted noise along the data they're designed to catch.
Not to mention that today, the variety of data is also large which leads into collecting data of different formats together.
Using \textit{dirty}, or invalid data can lead into making incorrect decisions. These can sometimes cause serious issues -
especially in health care and financing sides \cite{Ilyas2015}.

\textit{Data cleansing} is the solution to the above-mentioned problem. It's a process that contains steps for \textit{detecting} possible
\textit{errors} in data, and generating data \textit{transformations} to repair them. Data cleansing can be done either based on
\textit{qualitative} or \textit{quantitative} techniques.

Quantitative techniques focus on error detection and correction based on numerical attributes of data \cite{Hellerstein2008}.
If for example some data is visualised in two-dimensional space, it's fairly easy to detect \textit{outliers}, and thus possible errors in the data.
Outlier detection is an example of quantitative data \textit{analysis}. My seminar report focuses on the latter, qualitative techniques of data cleansing, and their scalability to big data.

Qualitative techniques are used to detect and correct data errors based on \textit{integrity constraints}, or \textit{patterns} in the data \cite{Ilyas2015}. In a database, an example of an integrity constraint could be such that each person should have unique social security number. A pattern on the other hand could be such that for each person, their ZIP code would define the city they live in. If we have several people with ZIP code being $00100$, and city as Helsinki, and yet another person with same ZIP code, but city being Turku, the latter mentioned person should be considered erroneous.

% TODO:
% Kirjoita alle eri frameworkeistä big datan käsittelyyn, ja miksi ovat hyödyllisiä.

The rest of the report is will be divided as follows. In section 2, I will summarise, where we are with big data cleansing at the moment.
I'll introduce several big data cleansing techniques in more detail. The techniques can practically be divided into \textit{de-duplication} methods, \textit{sampling}, \textit{incremental cleansing}, and \textit{distributed cleansing}.
I'll also discuss what kinds of problems are yet to be solved. % skaalautuvuus --> oletuksia, dataa usealla 'saitilla', holistic vs. non-holistic cleansing, ... ?

In section 3, I will go through actual implementations for big data cleansing techniques. % BigDansing, KATARA(?), ML frameworks, ... ?
% TODO: write more here
Section 4 discusses weak points in the papers, and in section 5, I show my own ideas to the research problem. Finally section 6 summarises the whole report. 



% 2. LITERATURE REVIEW
\section{Literature review}

- Määrittele ensin dirty / low quality data\\
- Seuraavaksi hyvä selittää, mistä qualitative data analysisissä ja sen eri tekniikoista on kyse

% 2015: TRENDS ja muiden artikkelien läpikäynti. Osan asioista voi jättää big datalle tarkoitettuun osioon.
% Selkeä aikajatkumo lähivuosien tapahtumista (ICs, frameworkejä näiden päälle, skaalautuminen big datan käsittelyyn (mm. BigDansing), tarkempaa tutkimusta työkalujen tarkkuudesta virheiden korjaukseen (2016: "Where are we now?"), työkalujen yhdistämistä tarkkuutta parantamalla, 2017: "HoloClean"
% 2017: CleanM (miten suhtautuu jatkumoon), 2017: CLAMS (menee jonkin verran asian ohi). Pitäisikö vain mainita, mutta ei mennä kovin tarkasti tämän ongelman ratkaisuun? Johtopäätösten yheydessä voi mainita ongelmasta unstructured datan käsittelyyn liittyen (samoin kuin se, että dataa usealla saitilla jota puhdistaa).



\section{Articles}

\textit{Quantitative Data Cleaning for Large Databases}

- Quanttitative data (outlier detection)
- Categorical data (same thing mentioned with a different name, misspellings)


\textit{An Efficient Data Cleaning Algorithm Based on Attributes Selection}

- SNM \& MPN algorithms for duplicate records detection
- Improved algorithm for duplicate error detection
- O(n log n)
- not suitable for big data?


\textit{Qualitative Data Cleaning}

- What kind of errors, how and where to detect them
- Data repairing
-- Trusting integrity constraints / rules, data or both?
-- Automatic / human guided (training ML model, suggesting fixes etc.)
-- Repairing on place / generating a model for repairing

\section{2015: TRENDS}

- Data deduplication can be seen as enforcing a key constraint defined on all the attributes of
a relational schema, since two duplicate tuples can be seen as a violation
of the key constraint.

\subsection{Error detection}

- A violation with respect to an IC is defined as the minimal subset
of database cells, such that at least one of the cells has to be modified
to satisfy the IC, where a cell is an attribute value of a tuple.

- Given a relational database instance I of schema R
and a set of integrity constraints ?, we need to find another database
instance I' with no violations with respect to ?.

- Given a dirty database instance, the first step toward cleaning the
database is to detect and surface anomalies or errors.

- Automation (How to Detect?): We classify proposed approaches
according to whether and how humans are involved in the
anomaly detection process. Most techniques are fully automatic,
for example, detecting violations of functional dependencies,
while other techniques involve humans, for example, to identify
duplicate records.

- Let ? denote a set of integrity constraints (ICs). We use I |= ?
to indicate that database Instance I conforms with the set of integrity
constraints ?. We say that I' is a repair of I with respect to ? if
I'  |=  ?, and CIDs(I) = CIDs(I') (i.e., no deletions or insertions are allowed to clean a database instance).

- Constraints (refer to p. 289 - 301)
-- FD, CFD, DC (subsumes the former so explaining them not useful?)

- Duplicate record detection
-- Similarity graph based on some similarity metric
-- Clustering enough similar records together (A <--> B and B <--> C    ---->  A <--> C)
--- In other words, finding connected components in the similarity graph
-- Merging the records into a single records (based on their attributes)


Holistic data cleaning (p. 318):
- Automatically detects violations based on multiple ICs

Where to detect (p. 322)
- Problematic that data errors are often found in business layers


\subsection{Error reparing}

- Repair target (What to repair?)
-- Repairing algorithms make different
assumptions about the data and the quality rules: (1) trusting
the declared integrity constraints, and hence, only data can
be updated to remove errors; (2) trusting the data completely
and allowing the relaxation of the constraints, for example, to
address schema evolution and obsolete business rules; and finally
(3) exploring the possibility of changing both the data and the constraints.

- -Holistic repairing (p. 336 - 342)
--- We describe these techniques as 'One at a time techniques'. Most available data repairing solutions are in this category.
They address one type of error, either to allow for theoretical quality guarantees, or to allow for a scalable system.

--- Multiple data quality problems, such as missing values, typos, the presence
of duplicate records, and business rule violations, are often observed
in a single data set. These heterogeneous types of errors interplay
and conflict on the same dataset, and treating them independently
would miss the opportunity to correctly identify the actual errors in the
data. We call the proposals that take a more holistic view of the data
cleaning process Holistic cleaning approaches. 

-- Mention also rules-only repairing, and both data and rules repairing (p. 345 - 350)
--- Continuous data cleansing


- Automation (How to repair?)
-- Fully automatic (trying to minimize cost between moving from I to I') vs. human involvement (verify / suggest fixes, train ML model)

-- Fully automatic: cardinality-minimal and cost-minimal repairs (p. 351)
--- - Unverified fixes, may introduce new errors during the process

-- Automatic data repairing techniques use heuristics, such as minimal
repairs to automatically repair the data in situ, and they often generate
unverified fixes. Worse still, they may even introduce new errors during
the process. It is often difficult, if not impossible, to guarantee the
accuracy of any data repairing techniques without external verification
via experts and trustworthy data sources. (p. 357)

-- Example 3.10. Consider two tuples t1 and t8 in Table 1.1; they both
have the same values ?25813? for ZIP attribute, but t1 has ?WA?
for ST attribute and t8 has ?WV? for ST attribute. Clearly, at least
one of the four cells t1[ZIP], t8[ZIP], t1[ST], t8[ST] has to be incorrect.
Lacking other evidence, existing automatic repairing techniques [17, 26]
often randomly choose one of the four cells to update. Some of them [17]
even limit the allowed changes to be t1[ST], t8[ST], since it is unclear
which values t1[ZIP], t8[ZIP] should take if they are to be changed. (p. 357)

-- Guided data repair
-- KATARA (scale-out?)
-- Data Tamer (scale-out?)


- Repair model (Where to repair?)
-- Repair database instance in place vs. generate a model for repairing the instance

-- Most of the proposed data repairing techniques 
(all discussed so far) identify errors in the data, and find a unique fix
of the data either by minimally modifying the data according to a cost
function or by using human guidance (Figure 3.17(a)). (p. 366)

-- As follows, we describe a different model-based approach for nondestructive
data cleaning. Data repairing techniques in this category do
not produce a single repair for a database instance; instead, they produce
a space of possible repairs (Figure 3.17(b)). The space of possible
repairs is used either to answer queries against the dirty data probabilistically
(e.g., using possible worlds semantics) [12], or to sample
from the space of all possible clean instances of the database [9, 11].

-- Probabilistic Cleaning
--- Probabilistic deduplication (probability based duplication "removal")

-- Sampling the Space of Possible Repairs
--- Relates closely to SampleClean for big data cleansing



BIG DATA CLEANING
- Pyrkimys vähentää tarvittavaa ihmiskommunikaatiota
- Mahd. vain datan pienen osajoukon käsittely ja tämän perusteella todennäköisyyksiin perustuvia vastauksia / jatkokäsittelyä

- Deduplicating
-- Blocking, windowing, canopy clustering

- Sampling (SampleClean)

- Incremental data cleaning
-- Entity resolution (ER) algorithm

- Distributed data cleaning
-- MapReduce, Spark, Dedoop (with ER) / Dis-Dedup, 
-- BigDansing (runs on top of a data processing platform like DBMS or MapReduce)
-- HoloClean?
-- CleanM ?

- Problems with data partitioned across multiple sites (p. 379)
-- Objective to minimize data shipment cost



%This section discusses background related to data cleansing.
%It introduces a general process for data cleansing containing
%data analysis and data transformation steps.

%It also introduces specific data quality related problems and
%different kinds of methods and frameworks for handling them.


% 5. BIG DATA CLEANSING
\section{Big data cleansing}

This section discusses problems that arise when handling big data.

- "Scalability. Large volumes of data render most current techniques unusable in real settings. "

This section also discusses recent frameworks designed particularly for handing big data.

I did further investigation with BigDansing, SampleClean, HoloClean, Dis-Dedup, and CleanM frameworks.
Ihab F. Ilyas has been part of research team of BigDansing, HoloClean and Dis-Dedup joined by
Xu Chu (with the exception of not being part of BigDansing research team) et al. and thus
I picked also CleanM for further investigation not to only have content from the same researchers.
Ilyas and Chu's have nevertheless conducted lots of essential research especially related to qualitative data cleansing.

TODO: I also discuss CLAMS which is not actually a big data cleansing framework, but rather one
for managing data quality via ICs with limited schema information (?).


BigDansing is a framework that can be run for example on top of
MapReduce to distribute error detection work.

HoloClean is 

Dis-Dedup is a framework for detecting duplicate data records. % improved version of DeDoop



% 6. WEAK POINTS ON PAPERS
\section{Weak points on papers}

This section discusses weak points of the papers related to the
papers current ongoing situation with big data cleansing.



% 7. OWN IDEAS
\section{Own ideas}

Give new ideas/algorithms/experiments on this research problem. This part is very important, because it
shows the potential of the author to be an independent innovative researcher. This section can be as long as
possible.

Come up with a new idea / algorithm that could be used for
dealing with big data and does something better than the existing
tools available...



% 8. CONCLUSION
\section{Conclusion}

Summarize the research problem and the main contributions of previous papers. The main weakness of
previous works could be also mentioned here. Some future works can be described as well.



% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{sigproc}  % sigproc.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references



% 10. EXAMPLES
\section{Examples}

\begin{table}
\centering
\caption{Frequency of Special Characters}
\begin{tabular}{|c|c|l|} \hline
Non-English or Math&Frequency&Comments\\ \hline
\O & 1 in 1,000& For Swedish names\\ \hline
$\pi$ & 1 in 5& Common in math\\ \hline
\$ & 4 in 5 & Used in business\\ \hline
$\Psi^2_1$ & 1 in 40,000& Unexplained usage\\
\hline\end{tabular}
\end{table}

\begin{table*}
\centering
\caption{Some Typical Commands}
\begin{tabular}{|c|c|l|} \hline
Command&A Number&Comments\\ \hline
\texttt{{\char'134}alignauthor} & 100& Author alignment\\ \hline
\texttt{{\char'134}numberofauthors}& 200& Author enumeration\\ \hline
\texttt{{\char'134}table}& 300 & For tables\\ \hline
\texttt{{\char'134}table*}& 400& For wider tables\\ \hline\end{tabular}
\end{table*}
end the environment with {table*}, NOTE not {table}!

As was the case with tables, you may want a figure
that spans two columns.  To do this, and still to
ensure proper ``floating'' placement of tables, use the environment
\textbf{figure*} to enclose the figure and its caption.
and don't forget to end the environment with
{figure*}, not {figure}!

\begin{figure*}
\centering
\includegraphics{flies}
\caption{A sample black and white graphic
that needs to span two columns of text.}
\end{figure*}


\begin{figure}
\centering
\includegraphics[height=1in, width=1in]{rosette}
\caption{A sample black and white graphic that has
been resized with the \texttt{includegraphics} command.}
\vskip -6pt
\end{figure}

\newtheorem{theorem}{Theorem}
\begin{theorem}
Let $f$ be continuous on $[a,b]$.  If $G$ is
an antiderivative for $f$ on $[a,b]$, then
\begin{displaymath}\int^b_af(t)dt = G(b) - G(a).\end{displaymath}
\end{theorem}

\newdef{definition}{Definition}
\begin{definition}
If $z$ is irrational, then by $e^z$ we mean the
unique number which has
logarithm $z$: \begin{displaymath}{\log e^z = z}\end{displaymath}
\end{definition}

\begin{proof}
Suppose on the contrary there exists a real number $L$ such that
\begin{displaymath}
\lim_{x\rightarrow\infty} \frac{f(x)}{g(x)} = L.
\end{displaymath}
Then
\begin{displaymath}
l=\lim_{x\rightarrow c} f(x)
= \lim_{x\rightarrow c}
\left[ g{x} \cdot \frac{f(x)}{g(x)} \right ]
= \lim_{x\rightarrow c} g(x) \cdot \lim_{x\rightarrow c}
\frac{f(x)}{g(x)} = 0\cdot L = 0,
\end{displaymath}
which contradicts our assumption that $l\neq 0$.
\end{proof}

\end{document}