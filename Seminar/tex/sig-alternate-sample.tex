% This is "sig-alternate.tex" V2.1 April 2013
% This file should be compiled with V2.5 of "sig-alternate.cls" May 2012
%
% This example file demonstrates the use of the 'sig-alternate.cls'
% V2.5 LaTeX2e document class file. It is for those submitting
% articles to ACM Conference Proceedings WHO DO NOT WISH TO
% STRICTLY ADHERE TO THE SIGS (PUBS-BOARD-ENDORSED) STYLE.
% The 'sig-alternate.cls' file will produce a similar-looking,
% albeit, 'tighter' paper resulting in, invariably, fewer pages.
%
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V2.5) produces:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) NO page numbers
%
% as against the acm_proc_article-sp.cls file which
% DOES NOT produce 1) thru' 3) above.
%
% Using 'sig-alternate.cls' you have control, however, from within
% the source .tex file, over both the CopyrightYear
% (defaulted to 200X) and the ACM Copyright Data
% (defaulted to X-XXXXX-XX-X/XX/XX).
% e.g.
% \CopyrightYear{2007} will cause 2007 to appear in the copyright line.
% \crdata{0-12345-67-8/90/12} will cause 0-12345-67-8/90/12 to appear in the copyright line.
%
% ---------------------------------------------------------------------------------------------------------------
% This .tex source is an example which *does* use
% the .bib file (from which the .bbl file % is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission, you *NEED* to 'insert'
% your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% ================= IF YOU HAVE QUESTIONS =======================
% Questions regarding the SIGS styles, SIGS policies and
% procedures, Conferences etc. should be sent to
% Adrienne Griscti (griscti@acm.org)
%
% Technical questions _only_ to
% Gerald Murray (murray@hq.acm.org)
% ===============================================================
%
% For tracking purposes - this is V2.0 - May 2012

\documentclass{sig-alternate-05-2015}

\begin{document}

\title{Scalable data cleansing based on qualitative attributes}

\numberofauthors{1}

\author{
\alignauthor
	Mika Huttunen\\
       		\affaddr{Helsinki University}
}

\date{11 March 2018}

\maketitle

\begin{abstract}
Abstact yet to be written..
\end{abstract}

%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below. 
%
%\begin{CCSXML}
%<ccs2012>
% <concept>
%  <concept_id>10010520.10010553.10010562</concept_id>
%  <concept_desc>Computer systems organization~Embedded systems</concept_desc>
%  <concept_significance>500</concept_significance>
% </concept>
% <concept>
%  <concept_id>10010520.10010575.10010755</concept_id>
%  <concept_desc>Computer systems organization~Redundancy</concept_desc>
%  <concept_significance>300</concept_significance>
% </concept>
%</ccs2012>  
%\end{CCSXML}

%\ccsdesc[500]{Computer systems organization~Embedded systems}
%\ccsdesc[300]{Computer systems organization~Redundancy}

\printccsdesc

\keywords{data cleansing, big data, scalability}


% 1. INTRODUCTION
\section{Introduction}

Nowadays companies are gathering large amounts of data in different ways such as via user input, and all kinds of sensors. It's also becoming easier and easier for them to use data in all kinds of decision making, analytics, and for example in automating human-involved tasks. Problems arise when the used data is \textit{dirty}, or invalid, and thus can lead into making incorrect decisions. These can sometimes cause serious issues - especially in health care and financing sides \cite{Ilyas2015}.

Humans often make data input errors via misspelling, and sensors may grab unwanted noise along the data they're designed to catch. In fact, over $25$\% of critical data in the world's top companies is flawed \cite{Khayyat2015}. Not to mention that today, the variety of data is also large which leads into collecting data of different formats together. \textit{Data cleansing} is the solution to the above-mentioned problem. For data controllers, understanding data cleansing, and the problems it tries to solve is thus naturally important. 

Data cleansing is a research field that explores ways to improve \textit{quality} of dirty data. If data isn't of high quality, it means that it has usually both \textit{schema}, and \textit{instance} level problems \cite{Rahm2000}. Another used definition for dirty data is that it doesn't meets its usage needs. A simple example of that would be such that a front developer of a service can't provide end-users a good UX because the server cannot deliver all kinds of useful data. This could happen when there are faults in the way the data is stored in the database.

% TODO: muuta lauseiden järjestystä
Figure \ref{rahm1} shows how \textit{single-source problems} with data quality can yet be divided into schema and instance level data problems. Single-source problems simply stand for such that can occur in data that is stored in a single site with possible data replication on the database side. The same problems can also arise in \textit{multi-source} systems where the data may also be scattered across several sites. %Today, there is no good solution for cleaning huge amounts of data in multi-source systems although some research towards this has been made \cite{Ilyas2015} (p. $379$).

\begin{figure}
	\centering
	\includegraphics[scale=0.8]{figures/rahm1.png}
	\caption{Single-source data quality problems \cite{Rahm2000}}
	\label{rahm1}
\end{figure}

Instance level problems in data arise as attributes of data \textit{tuples} being out of their scope, or just wrong. These contain misspellings, and \textit{outliers} by for example possible sensor errors, and processing errors before gathered data is actually stored. These problems can be detected via \textit{quantitative techniques} related to data cleansing that focus on error detection and correction based on numerical attributes of data \cite{Hellerstein2008}. If for example some framework can reduce multi-dimensional data in two-dimensional space, it's fairly easy for an expert to detect outliers in that space, and thus possible errors in the data.

Schema level problems on the other hand arise as \textit{violations} of \textit{rules} such as \textit{functional dependencies} (FD) \cite{Ilyas2015} (p. $289 - 291$). As an example, functional dependency
\begin{equation}
  ZIP \rightarrow STATE \label{eq1}
\end{equation} defines a constraint between data attributes $ZIP$ and $STATE$ so that $ZIP$ implies the $STATE$. In other words, if we had a dataset with two tuples having the same $ZIP$ attribute, but different values of $STATE$, and we trust rule \ref{eq1}, at least one of them is erroneous.

\textit{Duplicate} (or partically duplicate) data entries can also be considered both schema level, and instance level data quality problems \cite{Ilyas2015} (p. $283$). They usually appear as the same entries with some attributes having different values from each other, and thus should be classified as instance level problems. They can however be detected by similar rules that are used for other detecting schema level problems.

Consider us having a person register where each person has its name, phone number, address, and zip code defined. Now we could have tuples $t_1$ and $t_2$ with the same name, phone number, and address, but different zip codes. Tuples $t_1$ and $t_2$  could be detected being duplicate via a rule which defines that each tuple is unique by a combination of ($name$, $phone$ $number$, $address$). This would be possible for example via a rule such as \textit{UDF} (user-defined function).

In traditional ways, the data errors are detected by algorithms that compare data tuples trying to find rule violations. Rules can either be pre-defined by an expert, or automated code, or such that the data cleansing framework itself generates them by processing the data. The found errors can afterwards be fixed either via totally automatic, or (partially) human guided procedures.

This seminar report focuses mostly on \textit{qualitative} data cleansing frameworks that are highly scalable for \textit{big data}. They detect data errors via data quality rules such FDs, \textit{CFDs} (conditional functional dependencies), \textit{DCs} (denial constraints) which are explained in the paper by Ilyas and Chu \cite{Ilyas2015} (p. 287 - 301). I will leave further investigation of them up to the reader.

Some data cleansing frameworks like \textit{BigDansing} also apply UDFs alongside the traditional rules (FDs, CFDs and DCs) \cite{Khayyat2015}. UDFs extend traditional rules by allowing usage of more complex norms using procedural language. Using UDFs for error detection thus improves error detection accuracy compared to using just traditional data quality rules.

Some data cleansing frameworks on the other hand don't require cleansing the whole source data, but rather solve the problem of queries returning invalid results by other means. \textit{SampleClean} for instance applies sampling for improving answer quality for queries while having a dirty database instance on the background \cite{Wang2014}.

The rest of the report is divided as follows: Section $2$ introduces recent findings, and history related to qualitative big data cleansing. It also discusses, where we stand on the field at the moment. Section $3$ discusses recent frameworks introduced in Section $2$ in more detail. It does also analysis and comparison with them. Section $4$ discusses open challenges in big data cleansing field, and Section $5$ has my own ideas for future work. Section $6$ concludes the report.

% There should also be a separate section for more accurate introductions to different kinds of state-of-art frameworks (especially HoloClean). Alongside that, I should also have a section for discussing weak points in the papers. So far, I haven't really found anything else than somewhat bad inner organisation of different topics - especially in \cite{Ilyas2015}. And beside them, I would also of course add a section of more detailed analysis of the papers, and what there is left to be improved with data cleansing. As far as I understand, HoloClean is an awesome framework for big data cleansing tasks when all the data is located under a single source, or site. Problems however arise when applying data cleansing for multi-source data. This is especially because the shipment of huge amounts of data over the network is slow \cite{Ilyas2015} (p. $379$). 


%Section $3$ gives background for following sections by introducing different kinds of techniques related to handling big data, but also for getting more accurate data cleansing results. Section $4$ discusses recent frameworks for scalable data cleansing big data, and in Section 5, I have a brief conclusion to summarise the report.
% TODO: section 5: omat ideat, algoritmit yms., conclusion: section 6

% 2. RELATED WORK
\section{Related work}

Data cleansing has been an interesting topic for decades, and there has been a large amount of research on the field in the recent years,  Extensive summary from $2015$ by Ilyas and Chu discusses recent techniques on cleansing relational data \cite{Ilyas2015}. The summary introduces algorithms for finding different kinds of data quality rules based on a database instance, and its schema. Error detection is afterwards possible by looking for erroneous tuples with tuple-wise comparisons using one rule at a time.

For error repairing, Ilyas and Chu introduce a classification of different data repairing techniques \cite{Ilyas2015} (p. $330 - 332$). Based on the classification, they also discuss recent frameworks that apply different techniques. Figure \ref{chu2} represents data repairing techniques classification.

\begin{figure*}
	\centering
	\includegraphics[scale=0.8]{figures/chu2.png}
	\caption{Data repairing techniques \cite{Chu2016}}
     \label{chu2}
\end{figure*}

As we can see from the figure, we can have three different \textit{repair targets}. We can either repair data, rules, or both. So far, we have only been discussing data repairing which is about repairing data based on a set of trusted data quality rules. This report will bypass techniques for the other two targets, but the latter repair targets are yet mentioned next.

Rules only repairing stands for trusting that data is valid, and a set of predefined data quality rules should be modified so that they match to the data. Similarly trusting both data and rules is an approach for modifying both data and rules in such way that they eventually match with each other.

We can also see from figure \ref{chu2} that data repair target is yet being divided into two sub methods. There are methods for detecting and repairing data errors using one rule at a time, or doing the same thing \textit{holisticly}. A holistic method stands for using multiple rules for detection and repairing at the same time, and an example application of the technique is \textit{Holistic data cleaning} \cite{Chu2013}. It can be shown, that the holistic technique improves the error detection accuracy compared to non-holistic methods \cite{Ilyas2015} (p. $331$).\\

% Kerro jotain holistic ja one-way data cleansingistä (s. 337 muun muassa) ja tämän pohjalta mainitse, että BigDansing toimii holistisesti(?). Tämän jälkeen voi kertoa jotain samplingistä (ja sitä kautta mainita SampleClean), uudehkosta tutkimuksesta vuodelta 2016, jossa vertaillaan eri frameworkejä keskenään ja havaitusta tarpeesta kehittää parempi systeemi. HoloClean paper vuodelta 2017.

The following part of the report is yet to be written out. But the plan was to talk about techniques related to doing data cleansing with big data, and afterwards recent frameworks, that apply these techniques. There are ones like BigDansing (distributed rule-based error detection and correcting), SampleClean (probability based fixed data sampling while having dirty database on the background), DeDoop / Dis-Dedup (frameworks specifically designed for duplicate tuples detection) \cite{Chu2016-3}, and HoloClean \cite{Rekatsinas2017}. HoloClean is by far the most interesting of the above-mentioned frameworks. It offers holistic data repairing, and unifies a range of data repairing methods under a common framework.

Abedjan et al. show in their research paper from $2016$ that even though frameworks such as BigDansing are highly scalable, and can be applied to cleansing all kinds of datasets, they can repair around $36$\% of all possible errors - even with well-optimized configuration parameters. If an expert is however used to cleanse data by applying several frameworks in the right order for specific data cleansing tasks, much better repairing accuracy is met \cite{Abedjan2016}.

This specific research probably inspired partially the same research team in designing HoloClean. According to the Rekatsinas et al., HoloClean achieves over twice the repairing accuracy of existing state-of-art methods \cite{Rekatsinas2017}.

%Given a dataset $I$, and an empty set of traditional data quality rules $R$, the main problem in error detection phase is finding 


%Ilyas and Chu define violation respect to a rule $r$ being the minimal subset of database cells such that at least one of the cells has to be modified to satisfy $r$. Here cell stands for an attribute value of a tuple, or a row. The same definition can of course be extended to non-relational, but structured data that NoSQL document databases can consist of.


%the manual construction of $R$ is time-consuming, and requires lots of domain expertise. Thus having automatic tools for generation of $R$ based on $I$ is essential \cite{Chu2016-2}. Previously implemented $TANE$ and $FASTFD$ that Ilyas and Chu discuss \cite{Ilyas2015} can be used for finding FDs based on a relational data, and its schema, while $FASTDC$ can be used for finding DCs.


% Data cleansing has been an interesting topic for decades \cite{Khayyat2015} although no extensive research existed before the 21st century \cite{rahm2000}. In paper from $2000$ when NoSQL didn't exist yet, Rahm and Do \cite{rahm2000} discuss data cleansing especially as a part of \textit{ETL} process (extraction-transformation-loading). In ETL process, data is extracted from various sources, it's integrated together, and eventually loaded into a data warehose for further processing. The process involves data cleansing as part of both extraction and integration phases such that in extraction phase, each source should be cleansed separately, and in integration phase, data transformations are applied on the data to format the data in a uniform way. On this phase, duplicate records from various sources should also be tracked, and merged together.



% -----------------------
% KERRO NÄISTÄ

% Most data cleansing frameworks involve human interaction: either by training a ML classifier built-in the framework to solve conflicts on its own, or solving conflicts themselves

%Detecting, and removing (partially) duplicate entries is one of the key areas in data cleansing, and frameworks such as \textit{DeDoop} and \textit{Dis-Dedup} which is examined in section $4$ are specifically designed for their detection.

% 2015: TRENDS ja muiden artikkelien läpikäynti. Osan asioista voi jättää big datalle tarkoitettuun osioon.
% Selkeä aikajatkumo lähivuosien tapahtumista (ICs, frameworkejä näiden päälle, skaalautuminen big datan käsittelyyn (mm. BigDansing), tarkempaa tutkimusta työkalujen tarkkuudesta virheiden korjaukseen (2016: "Where are we now?"), työkalujen yhdistämistä tarkkuutta parantamalla, 2017: "HoloClean"
% 2017: CleanM (miten suhtautuu jatkumoon), 2017: CLAMS (menee jonkin verran asian ohi). Pitäisikö vain mainita, mutta ei mennä kovin tarkasti tämän ongelman ratkaisuun? Johtopäätösten yheydessä voi mainita ongelmasta unstructured datan käsittelyyn liittyen (samoin kuin se, että dataa usealla saitilla jota puhdistaa).

%The rest of the report is will be divided as follows. In section 2, I will summarise, where we are with big data cleansing at the moment. I'll introduce several big data cleansing techniques in more detail. The techniques can practically be divided into \textit{de-duplication} methods, \textit{sampling}, \textit{incremental cleansing}, and \textit{distributed cleansing}. I'll also discuss what kinds of problems are yet to be solved. 

% skaalautuvuus --> oletuksia, dataa usealla 'saitilla', holistic vs. non-holistic cleansing, ... ?
% -----------------------




% 2008
%\textit{Quantitative Data Cleaning for Large Databases}

%- Quantitative data (outlier detection)
%- Categorical data (same thing mentioned with a different name, misspellings)


% 2011
%\textit{An Efficient Data Cleaning Algorithm Based on Attributes Selection}

%- SNM \& MPN algorithms for duplicate records detection
%- Improved algorithm for duplicate error detection
%- O(n log n)
%- not suitable for big data?


%\textit{Qualitative Data Cleaning}

%- What kind of errors, how and where to detect them
%- Data repairing
%-- Trusting integrity constraints / rules, data or both?
%-- Automatic / human guided (training ML model, suggesting fixes etc.)
%-- Repairing on place / generating a model for repairing

%\section{2015: TRENDS}

%- Data deduplication can be seen as enforcing a key constraint defined on all the attributes of
%a relational schema, since two duplicate tuples can be seen as a violation
%of the key constraint.

%\subsection{Error detection}

%- Given a relational database instance I of schema R
%and a set of integrity constraints ?, we need to find another database
%instance I' with no violations with respect to ?.

%- Given a dirty database instance, the first step toward cleaning the
%database is to detect and surface anomalies or errors.

%- Automation (How to Detect?): We classify proposed approaches
%according to whether and how humans are involved in the
%anomaly detection process. Most techniques are fully automatic,
%for example, detecting violations of functional dependencies,
%while other techniques involve humans, for example, to identify
%duplicate records.

%- Let ? denote a set of integrity constraints (ICs). We use I |= ?
%to indicate that database Instance I conforms with the set of integrity
%constraints ?. We say that I' is a repair of I with respect to ? if
%I'  |=  ?, and CIDs(I) = CIDs(I') (i.e., no deletions or insertions are allowed to clean a database instance).

%- Constraints (refer to p. 289 - 301)
%-- FD, CFD, DC (subsumes the former so explaining them not useful?)

%- Duplicate record detection
%-- Similarity graph based on some similarity metric
%-- Clustering enough similar records together (A <--> B and B <--> C    ---->  A <--> C)
%--- In other words, finding connected components in the similarity graph
%-- Merging the records into a single records (based on their attributes)


%Holistic data cleaning (p. 318):
%- Automatically detects violations based on multiple ICs

%Where to detect (p. 322)
%- Problematic that data errors are often found in business layers


%\subsection{Error reparing}

%- Repair target (What to repair?)
%-- Repairing algorithms make different
%assumptions about the data and the quality rules: (1) trusting
%the declared integrity constraints, and hence, only data can
%be updated to remove errors; (2) trusting the data completely
%and allowing the relaxation of the constraints, for example, to
%address schema evolution and obsolete business rules; and finally
%(3) exploring the possibility of changing both the data and the constraints.

%- -Holistic repairing (p. 336 - 342)
%--- We describe these techniques as 'One at a time techniques'. Most available data repairing solutions are in this category.
%They address one type of error, either to allow for theoretical quality guarantees, or to allow for a scalable system.

%--- Multiple data quality problems, such as missing values, typos, the presence
%of duplicate records, and business rule violations, are often observed
%in a single data set. These heterogeneous types of errors interplay
%and conflict on the same dataset, and treating them independently
%would miss the opportunity to correctly identify the actual errors in the
%data. We call the proposals that take a more holistic view of the data
%cleaning process Holistic cleaning approaches. 

%-- Mention also rules-only repairing, and both data and rules repairing (p. 345 - 350)
%--- Continuous data cleansing


%- Automation (How to repair?)
%-- Fully automatic (trying to minimize cost between moving from I to I') vs. human involvement (verify / suggest fixes, train ML model)

%-- Fully automatic: cardinality-minimal and cost-minimal repairs (p. 351)
%--- - Unverified fixes, may introduce new errors during the process

%-- Automatic data repairing techniques use heuristics, such as minimal
%repairs to automatically repair the data in situ, and they often generate
%unverified fixes. Worse still, they may even introduce new errors during
%the process. It is often difficult, if not impossible, to guarantee the
%accuracy of any data repairing techniques without external verification
%via experts and trustworthy data sources. (p. 357)

%-- Example 3.10. Consider two tuples t1 and t8 in Table 1.1; they both
%have the same values ?25813? for ZIP attribute, but t1 has ?WA?
%for ST attribute and t8 has ?WV? for ST attribute. Clearly, at least
%one of the four cells t1[ZIP], t8[ZIP], t1[ST], t8[ST] has to be incorrect.
%Lacking other evidence, existing automatic repairing techniques [17, 26]
%often randomly choose one of the four cells to update. Some of them [17]
%even limit the allowed changes to be t1[ST], t8[ST], since it is unclear
%which values t1[ZIP], t8[ZIP] should take if they are to be changed. (p. 357)

%-- Guided data repair
%-- KATARA (scale-out?)
%-- Data Tamer (scale-out?)


%- Repair model (Where to repair?)
%-- Repair database instance in place vs. generate a model for repairing the instance

%-- Most of the proposed data repairing techniques 
%(all discussed so far) identify errors in the data, and find a unique fix
%of the data either by minimally modifying the data according to a cost
%function or by using human guidance (Figure 3.17(a)). (p. 366)

%-- As follows, we describe a different model-based approach for nondestructive
%data cleaning. Data repairing techniques in this category do
%not produce a single repair for a database instance; instead, they produce
%a space of possible repairs (Figure 3.17(b)). The space of possible
%repairs is used either to answer queries against the dirty data probabilistically
%(e.g., using possible worlds semantics) [12], or to sample
%from the space of all possible clean instances of the database [9, 11].

%-- Probabilistic Cleaning
%--- Probabilistic deduplication (probability based duplication "removal")

%-- Sampling the Space of Possible Repairs
%--- Relates closely to SampleClean for big data cleansing



%BIG DATA CLEANING
%- Pyrkimys vähentää tarvittavaa ihmiskommunikaatiota
%- Mahd. vain datan pienen osajoukon käsittely ja tämän perusteella todennäköisyyksiin perustuvia vastauksia / jatkokäsittelyä

%- Deduplicating
%-- Blocking, windowing, canopy clustering

%- Sampling (SampleClean)

%- Incremental data cleaning
%-- Entity resolution (ER) algorithm

%- Distributed data cleaning
%-- MapReduce, Spark, Dedoop (with ER) / Dis-Dedup, 
%-- BigDansing (runs on top of a data processing platform like DBMS or MapReduce)
%-- HoloClean?
%-- CleanM ?

%- Problems with data partitioned across multiple sites (p. 379)
%-- Objective to minimize data shipment cost



%This section discusses background related to data cleansing.
%It introduces a general process for data cleansing containing
%data analysis and data transformation steps.

%It also introduces specific data quality related problems and
%different kinds of methods and frameworks for handling them.


% 5. BIG DATA CLEANSING
%\section{Big data cleansing}

%This section discusses problems that arise when handling big data.

%- "Scalability. Large volumes of data render most current techniques unusable in real settings. "

%This section also discusses recent frameworks designed particularly for handing big data.

%I did further investigation with BigDansing, SampleClean, HoloClean, Dis-Dedup, and CleanM frameworks.
%Ihab F. Ilyas has been part of research team of BigDansing, HoloClean and Dis-Dedup joined by
%Xu Chu (with the exception of not being part of BigDansing research team) et al. and thus
%I picked also CleanM for further investigation not to only have content from the same researchers.
%Ilyas and Chu's have nevertheless conducted lots of essential research especially related to qualitative data cleansing.

%TODO: I also discuss CLAMS which is not actually a big data cleansing framework, but rather one
%for managing data quality via ICs with limited schema information (?).


%BigDansing is a framework that can be run for example on top of
%MapReduce to distribute error detection work.

%HoloClean is 

%Dis-Dedup is a framework for detecting duplicate data records. % improved version of DeDoop



% 6. WEAK POINTS ON PAPERS
%\section{Weak points on papers}

%This section discusses weak points of the papers related to the
%papers current ongoing situation with big data cleansing.



% 7. OWN IDEAS
%\section{Own ideas}

%Give new ideas/algorithms/experiments on this research problem. This part is very important, because it
%shows the potential of the author to be an independent innovative researcher. This section can be as long as
%possible.

%Come up with a new idea / algorithm that could be used for
%dealing with big data and does something better than the existing
%tools available...


% 3. STATE-OF-ART FRAMEWORKS
\section{State-of-art frameworks}

This section discusses the recent frameworks on big data cleansing field. I introduce BigDansing \cite{Khayyat2015}, SampleClean \cite{Wang2014}, Dis-Dedup \cite{Chu2016-3}, and HoloClean \cite{Rekatsinas2017}. Although the frameworks are quite different, some deeper analysis between them can be found at the end of this section.

% I introduce BigDansing that is a highly scalable framework for distributed data cleansing, SampleClean which avoids the problem of cleansing the actual whole data, Dis-Dedup, recent framework particularly for distributed duplicate data records detection, and HoloClean that unites a range of data repairing methods in a common framework.

\subsection{BigDansing}

BigDansing is a highly scalable framework for data cleansing. It makes parallelised error detection possible by providing an abstraction for data quality rules (FDs, CFDs, DCs and UDFs), and having an internal solution for efficiently handling abstracted rules. BigDansing can be run on top of a distributed computing system like MapReduce. \\

Discussion related to this and rest of the frameworks is yet to be written..


% 4. OPEN CHALLENGES
\section{Open challenges}

We have seen that there are highly scalable, and accurate frameworks for cleansing relational and structured data. There is however a large variety of data that doesn't fit into those categories like JSON and text documents, images, audio files and so on. Data quality problems related to them still remain unexplored \cite{Chu2016-2}.

Along with open challenges for data variety, velocity also poses a problem in big data cleansing. Even though the running times of recent frameworks for actual cleansing of large datasets are reasonable, they don't fit for continuos repairing of streaming data. Data sampling can however be applied for always having valid data for querying.


% 5. OWN IDEAS
\section{Own ideas}

Chu and Ilyas state that data quality problems related to cleansing semi-structured or unstructured data yet remain unexplored \cite{Chu2016-2}. I think that for the next steps in data cleansing, one should focus on this specific problem. Many used databases today are after all NoSQL, and store data in a way where rule-based error detection is not applicable.


% 6. CONCLUSION
\section{Conclusion}

Conclusion yet to be written..


% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{sigproc}  % sigproc.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references



% 10. EXAMPLES
%\section{Examples}

%\begin{table}
%\centering
%\caption{Frequency of Special Characters}
%\begin{tabular}{|c|c|l|} \hline
%Non-English or Math&Frequency&Comments\\ \hline
%\O & 1 in 1,000& For Swedish names\\ \hline
%$\pi$ & 1 in 5& Common in math\\ \hline
%\$ & 4 in 5 & Used in business\\ \hline
%$\Psi^2_1$ & 1 in 40,000& Unexplained usage\\
%\hline\end{tabular}
%\end{table}

%\begin{figure}
%\centering
%\includegraphics[height=1in, width=1in]{rosette}
%\caption{A sample black and white graphic that has
%been resized with the \texttt{includegraphics} command.}
%\vskip -6pt
%\end{figure}

%\newtheorem{theorem}{Theorem}
%\begin{theorem}
%Let $f$ be continuous on $[a,b]$.  If $G$ is
%an antiderivative for $f$ on $[a,b]$, then
%\begin{displaymath}\int^b_af(t)dt = G(b) - G(a).\end{displaymath}
%\end{theorem}

%\newdef{definition}{Definition}
%\begin{definition}
%If $z$ is irrational, then by $e^z$ we mean the
%unique number which has
%logarithm $z$: \begin{displaymath}{\log e^z = z}\end{displaymath}
%\end{definition}

%\begin{proof}
%Suppose on the contrary there exists a real number $L$ such that
%\begin{displaymath}
%\lim_{x\rightarrow\infty} \frac{f(x)}{g(x)} = L.
%\end{displaymath}
%Then
%\begin{displaymath}
%l=\lim_{x\rightarrow c} f(x)
%= \lim_{x\rightarrow c}
%\left[ g{x} \cdot \frac{f(x)}{g(x)} \right ]
%= \lim_{x\rightarrow c} g(x) \cdot \lim_{x\rightarrow c}
%\frac{f(x)}{g(x)} = 0\cdot L = 0,
%\end{displaymath}
%which contradicts our assumption that $l\neq 0$.
%\end{proof}

\end{document}