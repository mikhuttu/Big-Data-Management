% This is "sig-alternate.tex" V2.1 April 2013
% This file should be compiled with V2.5 of "sig-alternate.cls" May 2012
%
% This example file demonstrates the use of the 'sig-alternate.cls'
% V2.5 LaTeX2e document class file. It is for those submitting
% articles to ACM Conference Proceedings WHO DO NOT WISH TO
% STRICTLY ADHERE TO THE SIGS (PUBS-BOARD-ENDORSED) STYLE.
% The 'sig-alternate.cls' file will produce a similar-looking,
% albeit, 'tighter' paper resulting in, invariably, fewer pages.
%
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V2.5) produces:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) NO page numbers
%
% as against the acm_proc_article-sp.cls file which
% DOES NOT produce 1) thru' 3) above.
%
% Using 'sig-alternate.cls' you have control, however, from within
% the source .tex file, over both the CopyrightYear
% (defaulted to 200X) and the ACM Copyright Data
% (defaulted to X-XXXXX-XX-X/XX/XX).
% e.g.
% \CopyrightYear{2007} will cause 2007 to appear in the copyright line.
% \crdata{0-12345-67-8/90/12} will cause 0-12345-67-8/90/12 to appear in the copyright line.
%
% ---------------------------------------------------------------------------------------------------------------
% This .tex source is an example which *does* use
% the .bib file (from which the .bbl file % is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission, you *NEED* to 'insert'
% your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% ================= IF YOU HAVE QUESTIONS =======================
% Questions regarding the SIGS styles, SIGS policies and
% procedures, Conferences etc. should be sent to
% Adrienne Griscti (griscti@acm.org)
%
% Technical questions _only_ to
% Gerald Murray (murray@hq.acm.org)
% ===============================================================
%
% For tracking purposes - this is V2.0 - May 2012

\documentclass{sig-alternate-05-2015}

\begin{document}

\title{Big data cleansing based on qualitative attributes}

\numberofauthors{1}

\author{
\alignauthor
	Mika Huttunen\\
       		\affaddr{Helsinki University}
}

\date{11 March 2018}

\maketitle

\begin{abstract}

This paper provides a sample of a \LaTeX\ document which conforms,
somewhat loosely, to the formatting guidelines for
ACM SIG Proceedings. It is an {\em alternate} style which produces
a {\em tighter-looking} paper and was designed in response to
concerns expressed, by authors, over page-budgets.
It complements the document \textit{Author's (Alternate) Guide to
Preparing ACM SIG Proceedings Using \LaTeX$2_\epsilon$\ and Bib\TeX}.
This source file has been written with the intention of being
compiled under \LaTeX$2_\epsilon$\ and BibTeX.

\end{abstract}

%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below. 
%
%\begin{CCSXML}
%<ccs2012>
% <concept>
%  <concept_id>10010520.10010553.10010562</concept_id>
%  <concept_desc>Computer systems organization~Embedded systems</concept_desc>
%  <concept_significance>500</concept_significance>
% </concept>
% <concept>
%  <concept_id>10010520.10010575.10010755</concept_id>
%  <concept_desc>Computer systems organization~Redundancy</concept_desc>
%  <concept_significance>300</concept_significance>
% </concept>
%</ccs2012>  
%\end{CCSXML}

%\ccsdesc[500]{Computer systems organization~Embedded systems}
%\ccsdesc[300]{Computer systems organization~Redundancy}

\printccsdesc

\keywords{big data, data cleansing, data analysis, data transformation}


% 1. INTRODUCTION
\section{Introduction}

Nowadays companies are gathering large amounts of data, and its becoming easier to use data to help their businesses in all kinds of
decision making, analytics, or for example automating human-involved tasks.
Data can be collected in different ways like via user input and, all kinds of sensors.
Humans often make data input errors via misspelling, and sensors may grab unwanted noise along the data they're designed to catch.
Not to mention that today, the variety of data is also large which leads into collecting data of different formats together.
Using \textit{dirty}, or invalid data can lead into making incorrect decisions. These can sometimes cause serious issues -
especially in health care and financing sides \cite{Ilyas2015}.

\textit{Data cleansing} is the solution to the above-mentioned problem. It's a process that contains steps for \textit{detecting} possible
\textit{errors} in data, and generating data \textit{transformations} to repair them. Data cleansing can be done either based on
\textit{qualitative} or \textit{quantitative} techniques.

Quantitative techniques focus on error detection and correction based on numerical attributes of data \cite{Hellerstein2008}.
If for example some data is visualised in two-dimensional space, it's fairly easy to detect \textit{outliers}, and thus possible errors in the data.
Outlier detection is an example of quantitative data \textit{analysis}. My seminar report focuses on the latter, qualitative techniques of data cleansing, and their scalability to big data.

Qualitative techniques are used to detect and correct data errors based on \textit{integrity constraints}, or \textit{patterns} in the data \cite{Ilyas2015}. In a database, an example of an integrity constraint could be such that each person should have unique social security number. A pattern on the other hand could be such that for each person, their ZIP code would define the city they live in. If we have several people with ZIP code being $00100$, and city as Helsinki, and yet another person with same ZIP code, but city being Turku, the latter mentioned person should be considered erroneous.

% TODO:
% Kirjoita alle eri frameworkeist‰ big datan k‰sittelyyn, ja miksi ovat hyˆdyllisi‰.

The rest of the report is will be divided as follows. In section 2, I will summarise, where we are with big data cleansing at the moment.
I'll introduce several big data cleansing techniques in more detail. The techniques can practically be divided into \textit{de-duplication} methods, \textit{sampling}, \textit{incremental cleansing}, and \textit{distributed cleansing}.
I'll also discuss what kinds of problems are yet to be solved. % skaalautuvuus --> oletuksia, dataa usealla 'saitilla', holistic vs. non-holistic cleansing, ... ?

In section 3, I will go through actual implementations for big data cleansing techniques. % BigDansing, KATARA(?), ML frameworks, ... ?
% TODO: write more here
Section 4 discusses weak points in the papers, and in section 5, I show my own ideas to the research problem. Finally section 6 summarises the whole report. 



% 2. LITERATURE REVIEW
\section{Literature review}

- M‰‰rittele ensin dirty / low quality data\\
- Seuraavaksi hyv‰ selitt‰‰, mist‰ qualitative data analysisiss‰ ja sen eri tekniikoista on kyse



% Yhdist‰ seuraava osaksi INTRODUCTION / LITERATURE REVIEW osiota

% 3. BACKGROUND
\section{General data cleansing process}

This section discusses background related to data cleansing.
It introduces a general process for data cleansing containing
data analysis and data transformation steps.

It also introduces specific data quality related problems and
different kinds of methods and frameworks for handling them.

\textit{Quantitative Data Cleaning for Large Databases}

- Quanttitative data (outlier detection)
- Categorical data (same thing mentioned with a different name, misspellings)


\textit{An Efficient Data Cleaning Algorithm Based on Attributes Selection}

- SNM \& MPN algorithms for duplicate records detection
- Improved algorithm for duplicate error detection
- O(n log n)
- not suitable for big data?


\textit{Qualitative Data Cleaning}

- What kind of errors, how and where to detect them
- Data repairing
-- Trusting integrity constraints / rules, data or both?
-- Automatic / human guided (training ML model, suggesting fixes etc.)
-- Repairing on place / generating a model for repairing

From the actual paper / book:

- "Minimal cost for repair"


3.1 - WHAT TO REPAIR

- Trusting integrity constraints
-- NADEEF is a holistic repairing algorithm based on user-defined rules (p. 337)

- Trusting data
-- No examples necessary to be given rather than mentioning this kind of rules cleansing approach

- Trusting (= "changing") both data and constraints:


3.2 - HOW TO REPAIR

3.2.1 - Automatic Repairing

- Cardinality- / Cost-minimal repair
-- Algorithm 8 for automatic repair procedure
- Unverified fixes, may introduce new errors during the process


3.2.2 - Human guided repair

- Guided Data Repair uses ML classifier
- KATARA
- Data Tamer (very tied to ML)


3.3 - WHERE TO REPAIR

One-shot cleaning

- "Most of the proposed data repairing techniques
(\textit{all discussed so far}) identify errors in the data, and find a unique fix
of the data either by minimally modifying the data according to a cost
function or by using human guidance (Figure 3.17(a))."

Probabilistic Cleaning
- Probabilistic deduplication (probability based duplication "removal")


4. BIG DATA CLEANING
- Pyrkimys v‰hent‰‰ tarvittavaa ihmiskommunikaatiota
- Mahd. vain datan pienen osajoukon k‰sittely ja t‰m‰n perusteella todenn‰kˆisyyksiin perustuvia vastauksia / jatkok‰sittely‰

- Deduplicating
-- Blocking, windowing, canopy clustering

- Sampling (SampleClean)

- Incremental data cleaning
-- Entity resolution (ER) algorithm

- Distributed data cleaning
-- MapReduce, Spark, Dedoop (with ER)
-- BigDansing (runs on top of a data processing platform like DBMS or MapReduce)

- Problems with data partitioned across multiple sites (p. 379)
-- Objective to minimize data shipment cost


5. CONCLUSION

- "Scalability. Large volumes of data render most current techniques unusable in real settings. "




\subsection{Type Changes and {\subsecit Special} Characters}
We have already seen several typeface changes in this sample.  You
can indicate italicized words or phrases in your text with
the command \texttt{{\char'134}textit}.



% 4. ISSUES WITH BIG DATA

\section{Issues with big data}

This section discusses problems that arise when
data cleansing is done with big data.



% 5. BIG DATA CLEANSING FRAMEWORKS
\section{Big data cleansing frameworks}

This section discusses tools and frameworks designed
particularly for handing big data. There are BigDansing and
KATARA, but what else is available?



% 6. WEAK POINTS ON PAPERS
\section{Weak points on papers}

This section discusses weak points of the papers related to the
papers current ongoing situation with big data cleansing.



% 7. OWN IDEAS
\section{Own ideas}

Give new ideas/algorithms/experiments on this research problem. This part is very important, because it
shows the potential of the author to be an independent innovative researcher. This section can be as long as
possible.

Come up with a new idea / algorithm that could be used for
dealing with big data and does something better than the existing
tools available...



% 8. CONCLUSION
\section{Conclusion}

Summarize the research problem and the main contributions of previous papers. The main weakness of
previous works could be also mentioned here. Some future works can be described as well.

This paragraph will end the body of this sample document.
Remember that you might still have Acknowledgments or
Appendices; brief samples of these
follow.  There is still the Bibliography to deal with; and
we will make a disclaimer about that here: with the exception
of the reference to the \LaTeX\ book, the citations in
this paper are to articles which have nothing to
do with the present subject and are used as
examples only.



% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{sigproc}  % sigproc.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references



% 10. EXAMPLES
\section{Examples}

\begin{table}
\centering
\caption{Frequency of Special Characters}
\begin{tabular}{|c|c|l|} \hline
Non-English or Math&Frequency&Comments\\ \hline
\O & 1 in 1,000& For Swedish names\\ \hline
$\pi$ & 1 in 5& Common in math\\ \hline
\$ & 4 in 5 & Used in business\\ \hline
$\Psi^2_1$ & 1 in 40,000& Unexplained usage\\
\hline\end{tabular}
\end{table}

\begin{table*}
\centering
\caption{Some Typical Commands}
\begin{tabular}{|c|c|l|} \hline
Command&A Number&Comments\\ \hline
\texttt{{\char'134}alignauthor} & 100& Author alignment\\ \hline
\texttt{{\char'134}numberofauthors}& 200& Author enumeration\\ \hline
\texttt{{\char'134}table}& 300 & For tables\\ \hline
\texttt{{\char'134}table*}& 400& For wider tables\\ \hline\end{tabular}
\end{table*}
end the environment with {table*}, NOTE not {table}!

As was the case with tables, you may want a figure
that spans two columns.  To do this, and still to
ensure proper ``floating'' placement of tables, use the environment
\textbf{figure*} to enclose the figure and its caption.
and don't forget to end the environment with
{figure*}, not {figure}!

\begin{figure*}
\centering
\includegraphics{flies}
\caption{A sample black and white graphic
that needs to span two columns of text.}
\end{figure*}


\begin{figure}
\centering
\includegraphics[height=1in, width=1in]{rosette}
\caption{A sample black and white graphic that has
been resized with the \texttt{includegraphics} command.}
\vskip -6pt
\end{figure}

\newtheorem{theorem}{Theorem}
\begin{theorem}
Let $f$ be continuous on $[a,b]$.  If $G$ is
an antiderivative for $f$ on $[a,b]$, then
\begin{displaymath}\int^b_af(t)dt = G(b) - G(a).\end{displaymath}
\end{theorem}

\newdef{definition}{Definition}
\begin{definition}
If $z$ is irrational, then by $e^z$ we mean the
unique number which has
logarithm $z$: \begin{displaymath}{\log e^z = z}\end{displaymath}
\end{definition}

\begin{proof}
Suppose on the contrary there exists a real number $L$ such that
\begin{displaymath}
\lim_{x\rightarrow\infty} \frac{f(x)}{g(x)} = L.
\end{displaymath}
Then
\begin{displaymath}
l=\lim_{x\rightarrow c} f(x)
= \lim_{x\rightarrow c}
\left[ g{x} \cdot \frac{f(x)}{g(x)} \right ]
= \lim_{x\rightarrow c} g(x) \cdot \lim_{x\rightarrow c}
\frac{f(x)}{g(x)} = 0\cdot L = 0,
\end{displaymath}
which contradicts our assumption that $l\neq 0$.
\end{proof}

\end{document}